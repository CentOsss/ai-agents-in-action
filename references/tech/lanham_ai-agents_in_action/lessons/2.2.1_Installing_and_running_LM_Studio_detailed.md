# 2.2.1 Установка и запуск LM Studio

## Введение
LM Studio — это инструмент для локального запуска языковых моделей (LLM) на вашем компьютере. Он позволяет использовать современные модели (Llama, Mistral, Phi и др.) без необходимости отправлять данные в облако, что важно для приватности, скорости и экономии.

## 1. Преимущества локального запуска LLM
- Экономия средств: нет затрат на облачные запросы.
- Конфиденциальность: данные не покидают ваш компьютер.
- Автономность: работа без интернета.
- Минимизация задержек: быстрые ответы без сетевых задержек.
- Гибкость: возможность дообучения и интеграции с внутренними сервисами.

## 2. Установка LM Studio
- Скачайте LM Studio с официального сайта (https://lmstudio.ai/).
- Установите программу, следуя инструкциям для вашей ОС (Windows, macOS, Linux).
- После установки откройте LM Studio и выберите нужную модель (например, Llama, Mistral).

## 3. Запуск модели и получение API
- После выбора и загрузки модели запустите её в LM Studio.
- В настройках найдите локальный API-эндпоинт (обычно http://localhost:1234 или другой порт).
- Теперь вы можете отправлять запросы к модели через этот API из своих приложений.

## 4. Интеграция локальной LLM в проект
- В коде укажите адрес локального API вместо облачного.
- Пример (Python):
```python
import requests
response = requests.post(
    "http://localhost:1234/v1/chat/completions",
    json={
        "model": "название_модели",
        "messages": [
            {"role": "user", "content": "Привет!"}
        ]
    }
)
print(response.json()["choices"][0]["message"]["content"])
```

## 5. Практические аспекты
- Для работы с локальными LLM требуется достаточно ресурсов (CPU/GPU, память).
- Следите за обновлениями моделей и LM Studio.
- Используйте локальные LLM для задач с повышенными требованиями к приватности или при большом количестве запросов.

## Проверочные вопросы
1. Какие преимущества даёт запуск языковых моделей локально?
2. Для чего используется LM Studio и какие модели оно поддерживает?
3. Как интегрировать локальную LLM в свой проект?
4. В каких случаях локальные LLM предпочтительнее облачных? 